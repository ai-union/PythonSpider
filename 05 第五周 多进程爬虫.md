# 专栏第五周多进程爬虫

前面我们已经掌握了一些爬虫基本的技术，也实现了一些爬虫程序。但是随着数据量变大时，我们之前的爬虫的效率或者说执行速度就会出现问题，之前我们都是一条数据爬取完成后才继续下一条数据的爬取，这种模式我们通常称它为单线程或者串行爬虫。那么该如何改善呢？通过本章的学习你将掌握以下内容：

- 多线程：了解多线程的基本概念
- 多进程：了解多进程的概念
- 性能对比：通过一个爬虫案例对比它们之间的性能
- 多进程的使用

## 多线程与多进程

1. 多线程和多进程概述

   当计算机运行程序时，就会创建包含代码和状态的进程。这些进程会通过计算机的一个或多个CPU执行。不过，同一时刻每个CPU只会执行一个进程，然后不同进程间快速切换，给我们一种错觉，感觉好像多个程序在同时进行。

   一个进程中，程序的执行也是在不同的线程间进行切换，每个线程执行程序的不同部分。

   例如：有一个大型工厂，该工厂负责生产电脑，工厂有很多的车间用来生产不同的电脑部件。每个车间又有很多工人互相合作共享资源来生产某个电脑部件。这里的工厂相当于一个爬虫工程，每个车间相当于一个进程，每个工人就相当于线程。

2. 多进程的使用方法

   我们这里为大家介绍的是Python中的multiprocessing库，使用方法如下：

   ```python
   from multiprocessing import Pool
   pool = Pool(processes = 4) # 这个4代表着进程数
   pool.map(func, iterable) # func 为方法名， iterable为可迭代的参数
   ```

3.性能对比

​	我们这里以糗事百科的用户名称为例，分别对单进程，2进程，4进程的性能进行对比，代码如下：

```python
import requests
import re
import time
from multiprocessing import Pool


headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64)\
     AppleWebKit/537.36 (KHTML, like Gecko) Chrome/76.0.3809.132 Safari/537.36'}

def spyder(url):
    '''
    解析页面
    '''
    res = requests.get(url, headers = headers)
    ids = re.findall('<h2>(.*?)</h2>', res.text, re.S) # 获取用户名
    time.sleep(1)

if __name__ == "__main__":
    urls = ["https://www.qiushibaike.com/text/page/{}/".format(str(i)) for i in range(1, 10)]
    start_1 = time.time()
    for url in urls:
        spyder(url)        
    end_1 = time.time()
    print("单进程：",end_1 - start_1)

    start_2 = time.time()
    pool = Pool(processes = 2)
    pool.map(spyder, urls)
    end_2 = time.time()
    print("2进程：",end_2 - start_2)

    start_3 = time.time()
    pool = Pool(processes = 4)
    pool.map(spyder, urls)
    end_3 = time.time()
    print("2进程：",end_3 - start_3)
```

运行结果：

![1567265431802](https://github.com/ai-union/PythonSpyder/blob/master/img/1567265431802.png?raw=true)

这里只是获取了用户名的信息，大家可以将其他信息也爬取出来看看，动手试试看看能缩少多少时间。好了这周的内容就这么多，虽然内容不是很多，但却很重要，大家要多多练习。