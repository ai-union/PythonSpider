# 专栏第九周 表单与模拟登录

我们在生活中经常能碰到基于注册和登录的网站，这类网站很多内容对于尚未登录的用户并不开放，因此在爬虫程序编写中考虑账号登录的问题就显得十分重要了。那么我们今天就来了解一下常见的登录所需要的技术问题。

本篇文章你将了解到如下内容:

> - 表单与POST请求
> - Cookie以及Cookie在Python中的使用

## 表单与POST

在之前的爬虫中，程序基本只使用了HTTP 的GET操作，即仅通过程序去“读”网页中的数据，但在实际的网站浏览中还会大量的涉及到HTTP POST操作。

我们这里所说的表达就是只在HTML页面中的form元素。

![1569653221455](https://github.com/ai-union/PythonSpyder/blob/master/img/1569653221455.png?raw=true)

1. 发送表单数据

   在Python中使用requests库中的post()方法可以完成简单的POST操作，下面的代码就是一个最基本的用法：

   ```python
   import requests
   form_data = {'username':'root', 'password':'pwd123'}
   req = requests.post('http://xxxxx.com', data = form_data)
   ```

   > 这里的网站，用户信息均是假数据。

   我们来观察一下实际中的网站登录信息
   
   - 首先我们要获取到网站的登录请求地址
   
     ![1569681231891](https://github.com/ai-union/PythonSpyder/blob/master/img/1569681231891.png?raw=true)
   
     我们可以在提交表单时，看到这个地址。
   
   - 接下来看一下这个表单都提交了哪些内容
   
     ![1569681364026](https://github.com/ai-union/PythonSpyder/blob/master/img/1569681364026.png?raw=true)
   
     但是，大多数网站的登录表单都是进行了加密或者其他的形式进行了包装，以至于我们简单的模拟表单内容是无法正常登录的。这时就需要通过提交Cookie信息来进行模拟登录。记下来为大家说一下在Python中是如何利用Cookie来进行登录的。
   
   ## Cookie的概述及运用
   
   1. Cookie,是指网站为了辨别用户身份，进行session跟踪而存储在用户本地的数据。电商网站通常通过跟踪用户的Cookie信息，给用户提供推荐产品。同样这里也保存了用户的信息，因此我们便可以通过提交Cookie来模拟网站登录了。
   
   2. 那么如何获取Cookie信息呢？
   
      同样可以通过浏览器的开发者工具(F12)开获取到，如下图：
   
      ![1569682063882](https://github.com/ai-union/PythonSpyder/blob/master/img/1569682063882.png?raw=true)
   
   3. 在Python中的使用
   
      - 未加cookie的返回内容。
   
        ```python
        import requests
        header = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64)\
             AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.90 Safari/537.36'}
        req = requests.get("https://accounts.douban.com/passport/setting", headers=header)
        req.encoding = "UTF-8"
        print(req.text)
        ```
   
        ![1569683001422](https://github.com/ai-union/PythonSpyder/blob/master/img/1569683001422.png?raw=true)
   
        我们访问的是个人设置页面，但是从页面内容来看，应该是登录界面的内容。
   
      - 添加cookie的返回内容
   
        ```python
        import requests
        
        header = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64)\
             AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.90 Safari/537.36',
             'Cookie': '这里填写你们自己的cookie内容'}
        req = requests.get("https://accounts.douban.com/passport/setting", headers=header)
        req.encoding = "UTF-8"
        print(req.text)
        ```
   
        ![1569683170817](https://github.com/ai-union/PythonSpyder/blob/master/img/1569683170817.png?raw=true)
   
        我们通过页面返回内容可以看到，正是我们的设置界面。但是我们并没用在代码里进行页面登录的操作。这也证明了通过cookie是可以模拟登录的。
   
      ## 实战案例
   
      我们拿拉勾网来做一个练习，将我们之前的内容简单的串联一下。
   
      - 目标网站：https://www.lagou.com/
      - 实现内容：爬取【爬虫】相关工作，保存到MongoDB
   
      1. 分析页面
   
         ![1569684196456](https://github.com/ai-union/PythonSpyder/blob/master/img/1569684196456.png?raw=true)
   
         我们通过页面源代码可以看到这里没有职位信息，因此可以得出该网站使用AJAX技术来显示数据。
   
         > 注意这里不是通过F12来看，而是右键---> 查看源代码
   
      2. 获取url规则以及请求头，cookie相关信息
   
         ![1569684397046](https://github.com/ai-union/PythonSpyder/blob/master/img/1569684397046.png?raw=true)
   
         ![1569684428783](https://github.com/ai-union/PythonSpyder/blob/master/img/1569684428783.png?raw=true)
   
         ![1569684489904](https://github.com/ai-union/PythonSpyder/blob/master/img/1569684489904.png?raw=true)
   
         经过翻页我们得出，该页这个页面是通过POST请求，其中**pn**就是页面数。并且根据页面返回的内容我得知总页数为15
   
         ![1569684645930](https://github.com/ai-union/PythonSpyder/blob/master/img/1569684645930.png?raw=true)
   
      3. 代码实现
   
         这个网站的反爬比较有趣。大家自己可以动手试试。如果搞不定的话，欢迎进群交流。
   
         
   
      

